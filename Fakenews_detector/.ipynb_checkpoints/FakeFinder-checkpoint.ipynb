{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#import ftfy\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word, upos, lemma=None, xpos=None, feats=None, misc=None):\n",
    "        self.word = word\n",
    "        self.norm = strong_normalize(word)\n",
    "        self.lemma = lemma if lemma else \"_\"\n",
    "        self.upos = upos\n",
    "        self.xpos = xpos if xpos else \"_\"\n",
    "        self.xupos = self.upos + \"|\" + self.xpos\n",
    "        self.feats = feats if feats else \"_\"\n",
    "        self.feats_set = \"_\" #parse_features(self.feats)\n",
    "        self.misc = misc if misc else \"_\"\n",
    "        #self.lang = lang if lang else \"_\"\n",
    "    \n",
    "\n",
    "class FakeNews:\n",
    "    def __init__(self, id, source, label, cite, author, lang_code, title, content, date, references):\n",
    "        self.id = id\n",
    "        self.source = source\n",
    "        self.label = label\n",
    "        self.cite = cite\n",
    "        self.author = author\n",
    "        self.lang_code = lang_code\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.date = date\n",
    "        self.references = references\n",
    "        \n",
    "        self.title_tokenized = []\n",
    "        self.content_tokenized = []\n",
    "        \n",
    "        \n",
    "    def show(self):\n",
    "        print(\"ID: \", self.id)\n",
    "        print(\"Source: \", self.source)\n",
    "        print(\"Label: \", self.label)\n",
    "        print(\"Title: \", self.title)\n",
    "        print(\"Content: \", self.content)\n",
    "        \n",
    "    def tokenization(self):\n",
    "        tokenized = self.title.split(\" \")\n",
    "        self.title_tokenized = [Word(token, \"Noun\") for token in tokenized]\n",
    "        tokenized = self.content.split(\" \")\n",
    "        self.content_tokenized = [Word(token, \"Noun\") for token in tokenized]\n",
    "        #print(token_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fakenews(file, limit=1000000):\n",
    "    print(\"Read file: \", file)\n",
    "    data_list = []\n",
    "    \n",
    "    with open(file=file, encoding=\"UTF-8\") as file:\n",
    "        idx = 0\n",
    "        while True:\n",
    "            idx+=1\n",
    "            line = file.readline().split(\"\\t\")\n",
    "            \n",
    "            if line[0] is \"\":\n",
    "                print(\"Finish to read: \", len(data_list), \" sets\")\n",
    "                break\n",
    "            else:\n",
    "                data = FakeNews(line[0],line[1],line[2],line[3],line[4],line[5],line[6],line[7],line[8],line[9])\n",
    "                data.tokenization()\n",
    "                #data.tagging()\n",
    "                #data.parsing()\n",
    "                data_list.append(data)\n",
    "                if idx > limit:\n",
    "                    break\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def build_voca(data_list, cutoff=1):\n",
    "    print(\"build vocaburaries\")\n",
    "    \n",
    "    wordsCount = Counter()\n",
    "    charsCount = Counter()\n",
    "    uposCount = Counter()\n",
    "    labelCount = Counter()\n",
    "    \n",
    "    for data in data_list:\n",
    "        wordsCount.update([token.norm for token in data.title_tokenized])\n",
    "        wordsCount.update([token.norm for token in data.content_tokenized])\n",
    "        for token in data.title_tokenized:\n",
    "            charsCount.update(token.word)\n",
    "        for token in data.content_tokenized:\n",
    "            charsCount.update(token.word)\n",
    "        \n",
    "        uposCount.update([token.upos for token in data.title_tokenized])\n",
    "        uposCount.update([token.upos for token in data.content_tokenized])\n",
    "        \n",
    "        labelCount.update([data.label])\n",
    "    \n",
    "    wordsCount = Counter({w: i for w, i in wordsCount.items() if i >= cutoff})\n",
    "    print(\"Vocab containing {} words\".format(len(wordsCount)))\n",
    "    print(\"Charset containing {} chars\".format(len(charsCount)))\n",
    "    print(\"UPOS containing {} tags\".format(len(uposCount)), uposCount)\n",
    "    print(\"LABEL containing {} tags\".format(len(labelCount)), labelCount)\n",
    " \n",
    "    full_dictionary = {\n",
    "        \"vocab\": list(wordsCount.keys()),\n",
    "        \"wordfreq\": wordsCount,\n",
    "        \"charset\": list(charsCount.keys()),\n",
    "        \"charfreq\": charsCount,\n",
    "        \"upos\": list(uposCount.keys()),\n",
    "        \"uposfreq\": uposCount.keys(),\n",
    "        \"label\": list(labelCount.keys())\n",
    "    }\n",
    "    \n",
    "    return full_dictionary\n",
    "\n",
    "    \n",
    "def strong_normalize(word):\n",
    "    w = word.lower()\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", w)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"([^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"([^\\d][^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"``\", '\"', w)\n",
    "    w = re.sub(r\"''\", '\"', w)\n",
    "    w = re.sub(r\"\\d\", \"0\", w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list = read_fakenews(\"./corpus/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_dic = build_voca(data_list)\n",
    "#data_list[0].title_tokenized[0].upos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encode_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_word,\n",
    "                 dim_word,# = 100,\n",
    "                 num_extWord,\n",
    "                 dim_extWord,# = 100,\n",
    "                 num_pos,\n",
    "                 dim_pos,# = 50,\n",
    "                 num_label,\n",
    "                 dropout_ratio,# = 0.33,\n",
    "                 dim_lstm_hidden,# = 50,\n",
    "                 num_lstm_layer,# = 50,\n",
    "                 cuda_device# = 0\n",
    "                ):\n",
    "        super(Encode_model, self).__init__()\n",
    "        \n",
    "        self.dim_pos = dim_pos\n",
    "        self.dim_lstm_hidden = dim_lstm_hidden\n",
    "        self.num_lstm_layer = num_lstm_layer\n",
    "        \n",
    "        self.word_emb = nn.Embedding(num_embeddings=num_word + 3, embedding_dim=dim_word)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=num_pos + 3, embedding_dim=dim_pos) if dim_pos > 0 else None\n",
    "        \n",
    "        self.input_size = dim_word + dim_pos\n",
    "        self.enc_lstm = nn.LSTM(input_size=self.input_size, hidden_size=dim_lstm_hidden)\n",
    "        self.enc_lstm_hidden = self.init_hidden()\n",
    "        self.linear_classifier = nn.Linear(in_features=dim_lstm_hidden, out_features=num_label)\n",
    "        \n",
    "    def forward(self, word_seqs):\n",
    "        \n",
    "        word_seqs = torch.LongTensor(word_seqs)\n",
    "        word_vecs = self.word_emb(word_seqs)\n",
    "#        if torch.cuda.is_available() and self.cuda_device >= 0:\n",
    "#            word_vecs.cuda()\n",
    "        \n",
    "        if self.dim_pos > 0:\n",
    "            pos_vecs = self.pos_emb(pos_seq)\n",
    "            input_vecs = torch.cat(word_vecs, pos_vecs)\n",
    "        else:\n",
    "            input_vecs = word_vecs\n",
    "            \n",
    "        lstm_out, lstm_hidden = self.enc_lstm(word_vecs.view(len(word_vecs), 1, -1), self.enc_lstm_hidden)\n",
    "        #print(lstm_out)\n",
    "        #print(lstm_hidden)\n",
    "        lin_out = self.linear_classifier(lstm_hidden[0])\n",
    "        #lin_out2 = self.linear_classifier(lstm_out.view(len(word_seqs), -1))\n",
    "\n",
    "        return lin_out.squeeze()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_lstm_layer, 1, self.dim_lstm_hidden),\n",
    "                torch.zeros(self.num_lstm_layer, 1, self.dim_lstm_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fakefinder:\n",
    "    def __init__(self, train_file, test_file, n_read, cuda_device=-1):\n",
    "        \n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.train_data_list = read_fakenews(self.train_file, limit=n_read)\n",
    "        self.test_data_list = read_fakenews(self.test_file, limit=n_read)\n",
    "        self.full_dic = build_voca(self.train_data_list)\n",
    "        self._set_vocab(self.full_dic)\n",
    "        \n",
    "        self.num_word = len(self._vocab)\n",
    "        self.dim_word = 100\n",
    "        self.num_extWord = len(self._vocab)\n",
    "        self.dim_extWord = 100\n",
    "        self.num_pos = len(self._upos)\n",
    "        self.dim_pos = 0\n",
    "        self.num_label = 3\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "        self.dropout_ratio = 0.33\n",
    "        self.dim_lstm_hidden = 200\n",
    "        self.num_lstm_layer = 1\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "        self.enc_model = Encode_model(\n",
    "                                    num_word = self.num_word,\n",
    "                                    dim_word = self.dim_word,\n",
    "                                    num_extWord = self.num_extWord,\n",
    "                                    dim_extWord = self.dim_extWord,\n",
    "                                    num_pos = self.num_pos ,\n",
    "                                    dim_pos = self.dim_pos,\n",
    "                                    num_label = self.num_label,\n",
    "                                    dropout_ratio = self.dropout_ratio,\n",
    "                                    dim_lstm_hidden = self.dim_lstm_hidden,\n",
    "                                    num_lstm_layer = self.num_lstm_layer,\n",
    "                                    cuda_device = self.cuda_device)\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.enc_model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available() and self.cuda_device >= 0:\n",
    "            self.enc_model.cuda(self.cuda_device)\n",
    "    \n",
    "    def _set_vocab(self, vocab):\n",
    "    \n",
    "        self._fullvocab = vocab\n",
    "        self._upos = {p: i for i, p in enumerate(vocab[\"upos\"])}\n",
    "        self._iupos = vocab[\"upos\"]\n",
    "        self._vocab = {w: i + 3 for i, w in enumerate(vocab[\"vocab\"])}\n",
    "        self._wordfreq = vocab[\"wordfreq\"]\n",
    "        self._charset = {c: i + 3 for i, c in enumerate(vocab[\"charset\"])}\n",
    "        self._charfreq = vocab[\"charfreq\"]\n",
    "        self._label = {w: i for i, w in enumerate(vocab[\"label\"])}\n",
    "        self._ilabel = vocab[\"label\"]\n",
    "        \n",
    "        self._vocab['*pad*'] = 0\n",
    "        self._charset['*pad*'] = 0\n",
    "        \n",
    "        self._vocab['*root*'] = 1\n",
    "        self._charset['*root*'] = 1\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        \n",
    "        print(\"Running train sets\")    \n",
    "        self.enc_model.train()\n",
    "        num_train = 0\n",
    "        num_accurate = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for data in self.train_data_list:\n",
    "            self.enc_model.zero_grad()\n",
    "            num_train+=1\n",
    "            \n",
    "            words = [token.norm for token in data.content_tokenized]\n",
    "            word_seqs = [self._vocab.get(token.norm, 0) for token in data.content_tokenized]\n",
    "            label = [self._label.get(data.label)]\n",
    "            #print(words[0])\n",
    "            #print(word_seqs)\n",
    "            #print(label_seq)\n",
    "            \n",
    "            logists = self.enc_model(word_seqs)\n",
    "            #print(logists)\n",
    "            loss = self.compute_loss(logists, label)\n",
    "            accurate = self.compute_accuracy(logists, label) \n",
    "            num_accurate += accurate\n",
    "            \n",
    "            #print(\"accuracy\", accuracy)\n",
    "            #print(\"loss\", loss.data[0])\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        print(\"Training loss: \", round(total_loss,2),  \"(\",num_train,\"/\",num_accurate.item(),\")\", \"  accuracy: \", round((num_accurate.item()/num_train)*100,2) )\n",
    "\n",
    "            \n",
    "    def compute_loss(self, pred_logist, gold):\n",
    "        \n",
    "        #gold = gold[0]\n",
    "        #gold = np.eye(3)[gold]\n",
    "        gold = torch.LongTensor(gold)\n",
    "        #print(gold)\n",
    "        #print(pred_logist.squeeze())\n",
    "        #print(F.softmax(pred_logist.squeeze()))\n",
    "        pred = pred_logist.unsqueeze(dim=0)\n",
    "        #print(pred)\n",
    "        #loss = F.cross_entropy(pred_logist.squeeze(), F.softmax(gold))\n",
    "        loss = self.criterion(pred, gold)\n",
    "        return loss\n",
    "    \n",
    "    def compute_accuracy(self, pred_logist, gold):\n",
    "        \n",
    "        #value, predicted = torch.max(pred_logist.data, 1)\n",
    "        #print(pred_logist.max())\n",
    "        #print(pred_logist.max(0))\n",
    "        pred = pred_logist.data.max(0)[1].cpu()\n",
    "        #print(\"predicted \" ,pred.data)\n",
    "        #print(\"predicted2 \" ,pred.data[0].cpu())\n",
    "        #print(\"gold \" ,gold[0])\n",
    "        predicted = (pred == gold[0]) if gold[0] is not None else 0\n",
    "        return predicted\n",
    "\n",
    "    def test(self, test_file=None):\n",
    "        print(\"Running test sets\")\n",
    "        self.enc_model.train()\n",
    "        \n",
    "        num_test = 0\n",
    "        num_accurate = 0\n",
    "        \n",
    "        test_data_list = read_fakenews(test_file) if self.test_data_list is None else self.test_data_list\n",
    "        #print(test_data_list[0].show())\n",
    "        for data in test_data_list:\n",
    "            num_test+=1\n",
    "            \n",
    "            words = [token.norm for token in data.content_tokenized]\n",
    "            word_seqs = [self._vocab.get(token.norm, 0) for token in data.content_tokenized]\n",
    "            label = [self._label.get(data.label)]\n",
    "            #print(label)\n",
    "\n",
    "            logists = self.enc_model(word_seqs)\n",
    "            #print(logists)\n",
    "            accurate = self.compute_accuracy(logists, label)\n",
    "            #print(accurate)\n",
    "            num_accurate += accurate\n",
    "\n",
    "        print(\"Testing results: \", \"(\",num_test,\"/\",num_accurate.item(),\")\", \"  accuracy: \", round((num_accurate.item()/num_test)*100,2) )\n",
    "\n",
    "        \n",
    "    def test_input(self, title, content):\n",
    "        print(\"Running with an input text\")\n",
    "        self.enc_model.eval()\n",
    "        \n",
    "        num_test = 0\n",
    "        num_accurate = 0\n",
    "        #input_data = FakeNews(line[0],line[1],line[2],line[3],line[4],line[5],line[6],line[7],line[8],line[9])\n",
    "        input_data = FakeNews(\"\",\"\",\"\",\"\",\"\",\"\",\"\",content,title,\"\")\n",
    "        input_data.tokenization()\n",
    "        \n",
    "        test_data_list = [input_data]\n",
    "        print(test_data_list[0].show())\n",
    "        for data in test_data_list:\n",
    "            num_test+=1\n",
    "            \n",
    "            words = [token.norm for token in data.content_tokenized]\n",
    "            word_seqs = [self._vocab.get(token.norm, 0) for token in data.content_tokenized]\n",
    "\n",
    "            logists = self.enc_model(word_seqs)\n",
    "            print(\"logists\", logists)\n",
    "            softLog = F.softmax(logists)\n",
    "            print(\"예측값:\", softLog)\n",
    "            #print(softLog.cpu()[0])\n",
    "            #print(softLog[1].item())\n",
    "            for idx, label in enumerate(self._ilabel):\n",
    "                print(label,\":\", round(softLog[idx].item(),3))\n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file:  ./corpus/train_ko.txt\n",
      "Finish to read:  10  sets\n",
      "Read file:  ./corpus/dev_ko.txt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2454157beead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfakefinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFakefinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./corpus/train_ko.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./corpus/dev_ko.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_read\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f234209ebe42>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_file, test_file, n_read, cuda_device)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_fakenews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_fakenews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_voca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-efc83c557666>\u001b[0m in \u001b[0;36mread_fakenews\u001b[0;34m(file, limit)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFakeNews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;31m#data.tagging()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "fakefinder = Fakefinder(\"./corpus/train_ko.txt\", \"./corpus/dev.txt\", n_read=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "while range(100):\n",
    "    fakefinder.train()\n",
    "    fakefinder.test()\n",
    "    fakefinder.test_input(\"배달의민족 통한 주문액 작년 기준으로 5.2조원\",\n",
    "                          \"배달의민족이 20조원에 달하는 국내 배달앱 시장의 4분의 1을 차지하는 것으로 조사됐다. 22일 배달의민족을 운영하는 우아한형제들에 따르면 배민에서 광고하는 외식 자영업들의 작년 매출은 총 5조2천억원으로 전년대비 73% 증가했다. 전체 배달앱 시장은 20조원 규모로 성장했다. 업주 한 명이 배민을 통해 올린 작년 월 평균 매출액은 650만원으로 전년500만원에서 약 30% 증가했다. 작년말 기준 배민 월 이용자수는 900만명, 월 주문수는 2천800만건을 넘어섰다. 전년대비 각각 50% 이상 증가했다. 이같은 매출 신장은 1인 가구, 맞벌이 부부, 밀레니얼 세대 등 인구 변화와 고객 트렌드의 변화, 여기에 폭염, 혹한, 미세먼지 등 환경 요인도 영향을 미친 것으로 보인다. 업주 대다수는 월 정액 광고를 이용 중이다. 2017년 1월 약 4만4천명이던 배민 광고주 수는 작년 말 약 8만명으로 2년 사이 두 배 가까이 늘었다. 국내 배달음식 시장 20조 원 이상으로 성장했다. 이 중 배달의민족이 전체 거래의 약 4분의 1을 차지한다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
