{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#import ftfy\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word, upos, lemma=None, xpos=None, feats=None, misc=None):\n",
    "        self.word = word\n",
    "        self.norm = strong_normalize(word)\n",
    "        self.lemma = lemma if lemma else \"_\"\n",
    "        self.upos = upos\n",
    "        self.xpos = xpos if xpos else \"_\"\n",
    "        self.xupos = self.upos + \"|\" + self.xpos\n",
    "        self.feats = feats if feats else \"_\"\n",
    "        self.feats_set = \"_\" #parse_features(self.feats)\n",
    "        self.misc = misc if misc else \"_\"\n",
    "        #self.lang = lang if lang else \"_\"\n",
    "    \n",
    "\n",
    "class FakeNews:\n",
    "    def __init__(self, id, source, label, cite, author, lang_code, title, content, date, references):\n",
    "        self.id = id\n",
    "        self.source = source\n",
    "        self.label = label\n",
    "        self.cite = cite\n",
    "        self.author = author\n",
    "        self.lang_code = lang_code\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.date = date\n",
    "        self.references = references\n",
    "        \n",
    "        self.title_tokenized = []\n",
    "        self.content_tokenized = []\n",
    "        \n",
    "        \n",
    "    def show(self):\n",
    "        print(\"ID: \", self.id)\n",
    "        print(\"Source: \", self.source)\n",
    "        print(\"Label: \", self.label)\n",
    "        print(\"Title: \", self.title)\n",
    "        print(\"Content: \", self.content)\n",
    "        \n",
    "    def tokenization(self):\n",
    "        tokenized = self.title.split(\" \")\n",
    "        self.title_tokenized = [Word(token, \"Noun\") for token in tokenized]\n",
    "        tokenized = self.content.split(\" \")\n",
    "        self.content_tokenized = [Word(token, \"Noun\") for token in tokenized]\n",
    "        #print(token_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fakenews(file, limit=1000000):\n",
    "    print(\"Read file: \", file)\n",
    "    data_list = []\n",
    "    \n",
    "    with open(file=file, encoding=\"UTF-8\") as file:\n",
    "        idx = 0\n",
    "        while True:\n",
    "            idx+=1\n",
    "            line = file.readline().split(\"\\t\")\n",
    "            \n",
    "            if line[0] is \"\":\n",
    "                print(\"Finish to read: \", len(data_list), \" sets\")\n",
    "                break\n",
    "            else:\n",
    "                data = FakeNews(line[0],line[1],line[2],line[3],line[4],line[5],line[6],line[7],line[8],line[9])\n",
    "                data.tokenization()\n",
    "                #data.tagging()\n",
    "                #data.parsing()\n",
    "                data_list.append(data)\n",
    "                if idx > limit:\n",
    "                    break\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def build_voca(data_list, cutoff=1):\n",
    "    print(\"build vocaburaries\")\n",
    "    \n",
    "    wordsCount = Counter()\n",
    "    charsCount = Counter()\n",
    "    uposCount = Counter()\n",
    "    labelCount = Counter()\n",
    "    \n",
    "    for data in data_list:\n",
    "        wordsCount.update([token.norm for token in data.title_tokenized])\n",
    "        wordsCount.update([token.norm for token in data.content_tokenized])\n",
    "        for token in data.title_tokenized:\n",
    "            charsCount.update(token.word)\n",
    "        for token in data.content_tokenized:\n",
    "            charsCount.update(token.word)\n",
    "        \n",
    "        uposCount.update([token.upos for token in data.title_tokenized])\n",
    "        uposCount.update([token.upos for token in data.content_tokenized])\n",
    "        \n",
    "        labelCount.update([data.label])\n",
    "    \n",
    "    wordsCount = Counter({w: i for w, i in wordsCount.items() if i >= cutoff})\n",
    "    print(\"Vocab containing {} words\".format(len(wordsCount)))\n",
    "    print(\"Charset containing {} chars\".format(len(charsCount)))\n",
    "    print(\"UPOS containing {} tags\".format(len(uposCount)), uposCount)\n",
    "    print(\"LABEL containing {} tags\".format(len(labelCount)), labelCount)\n",
    " \n",
    "    full_dictionary = {\n",
    "        \"vocab\": list(wordsCount.keys()),\n",
    "        \"wordfreq\": wordsCount,\n",
    "        \"charset\": list(charsCount.keys()),\n",
    "        \"charfreq\": charsCount,\n",
    "        \"upos\": list(uposCount.keys()),\n",
    "        \"uposfreq\": uposCount.keys(),\n",
    "        \"label\": list(labelCount.keys())\n",
    "    }\n",
    "    \n",
    "    return full_dictionary\n",
    "\n",
    "    \n",
    "def strong_normalize(word):\n",
    "    w = word.lower()\n",
    "    w = re.sub(r\".+@.+\", \"*EMAIL*\", w)\n",
    "    w = re.sub(r\"@\\w+\", \"*AT*\", w)\n",
    "    w = re.sub(r\"(https?://|www\\.).*\", \"*url*\", w)\n",
    "    w = re.sub(r\"([^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"([^\\d][^\\d])\\1{2,}\", r\"\\1\\1\", w)\n",
    "    w = re.sub(r\"``\", '\"', w)\n",
    "    w = re.sub(r\"''\", '\"', w)\n",
    "    w = re.sub(r\"\\d\", \"0\", w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list = read_fakenews(\"./corpus/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_dic = build_voca(data_list)\n",
    "#data_list[0].title_tokenized[0].upos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encode_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_word,\n",
    "                 dim_word,# = 100,\n",
    "                 num_extWord,\n",
    "                 dim_extWord,# = 100,\n",
    "                 num_pos,\n",
    "                 dim_pos,# = 50,\n",
    "                 num_label,\n",
    "                 dropout_ratio,# = 0.33,\n",
    "                 dim_lstm_hidden,# = 50,\n",
    "                 num_lstm_layer,# = 50,\n",
    "                 cuda_device# = 0\n",
    "                ):\n",
    "        super(Encode_model, self).__init__()\n",
    "        \n",
    "        self.dim_pos = dim_pos\n",
    "        self.dim_lstm_hidden = dim_lstm_hidden\n",
    "        self.num_lstm_layer = num_lstm_layer\n",
    "        \n",
    "        self.word_emb = nn.Embedding(num_embeddings=num_word + 3, embedding_dim=dim_word)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=num_pos + 3, embedding_dim=dim_pos) if dim_pos > 0 else None\n",
    "        \n",
    "        self.input_size = dim_word + dim_pos\n",
    "        self.enc_lstm = nn.LSTM(input_size=self.input_size, hidden_size=dim_lstm_hidden)\n",
    "        self.enc_lstm_hidden = self.init_hidden()\n",
    "        self.linear_classifier = nn.Linear(in_features=dim_lstm_hidden, out_features=num_label)\n",
    "        \n",
    "    def forward(self, word_seqs):\n",
    "        \n",
    "        word_seqs = torch.LongTensor(word_seqs)\n",
    "        word_vecs = self.word_emb(word_seqs)\n",
    "#        if torch.cuda.is_available() and self.cuda_device >= 0:\n",
    "#            word_vecs.cuda()\n",
    "        \n",
    "        if self.dim_pos > 0:\n",
    "            pos_vecs = self.pos_emb(pos_seq)\n",
    "            input_vecs = torch.cat(word_vecs, pos_vecs)\n",
    "        else:\n",
    "            input_vecs = word_vecs\n",
    "            \n",
    "        lstm_out, lstm_hidden = self.enc_lstm(word_vecs.view(len(word_vecs), 1, -1), self.enc_lstm_hidden)\n",
    "        #print(lstm_out)\n",
    "        #print(lstm_hidden)\n",
    "        lin_out = self.linear_classifier(lstm_hidden[0])\n",
    "        #lin_out2 = self.linear_classifier(lstm_out.view(len(word_seqs), -1))\n",
    "\n",
    "        return lin_out.squeeze()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_lstm_layer, 1, self.dim_lstm_hidden),\n",
    "                torch.zeros(self.num_lstm_layer, 1, self.dim_lstm_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fakefinder:\n",
    "    def __init__(self, train_file, test_file, n_read, cuda_device=-1):\n",
    "        \n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.train_data_list = read_fakenews(self.train_file, limit=n_read)\n",
    "        self.test_data_list = read_fakenews(self.test_file, limit=n_read)\n",
    "        self.full_dic = build_voca(self.train_data_list)\n",
    "        self._set_vocab(self.full_dic)\n",
    "        \n",
    "        self.num_word = len(self._vocab)\n",
    "        self.dim_word = 100\n",
    "        self.num_extWord = len(self._vocab)\n",
    "        self.dim_extWord = 100\n",
    "        self.num_pos = len(self._upos)\n",
    "        self.dim_pos = 0\n",
    "        self.num_label = 3\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "        self.dropout_ratio = 0.33\n",
    "        self.dim_lstm_hidden = 200\n",
    "        self.num_lstm_layer = 1\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "        self.enc_model = Encode_model(\n",
    "                                    num_word = self.num_word,\n",
    "                                    dim_word = self.dim_word,\n",
    "                                    num_extWord = self.num_extWord,\n",
    "                                    dim_extWord = self.dim_extWord,\n",
    "                                    num_pos = self.num_pos ,\n",
    "                                    dim_pos = self.dim_pos,\n",
    "                                    num_label = self.num_label,\n",
    "                                    dropout_ratio = self.dropout_ratio,\n",
    "                                    dim_lstm_hidden = self.dim_lstm_hidden,\n",
    "                                    num_lstm_layer = self.num_lstm_layer,\n",
    "                                    cuda_device = self.cuda_device)\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.enc_model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if torch.cuda.is_available() and self.cuda_device >= 0:\n",
    "            self.enc_model.cuda(self.cuda_device)\n",
    "    \n",
    "    def _set_vocab(self, vocab):\n",
    "    \n",
    "        self._fullvocab = vocab\n",
    "        self._upos = {p: i for i, p in enumerate(vocab[\"upos\"])}\n",
    "        self._iupos = vocab[\"upos\"]\n",
    "        self._vocab = {w: i + 3 for i, w in enumerate(vocab[\"vocab\"])}\n",
    "        self._wordfreq = vocab[\"wordfreq\"]\n",
    "        self._charset = {c: i + 3 for i, c in enumerate(vocab[\"charset\"])}\n",
    "        self._charfreq = vocab[\"charfreq\"]\n",
    "        self._label = {w: i for i, w in enumerate(vocab[\"label\"])}\n",
    "        self._ilabel = vocab[\"label\"]\n",
    "        \n",
    "        self._vocab['*pad*'] = 0\n",
    "        self._charset['*pad*'] = 0\n",
    "        \n",
    "        self._vocab['*root*'] = 1\n",
    "        self._charset['*root*'] = 1\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        \n",
    "        print(\"Running train sets\")    \n",
    "        self.enc_model.train()\n",
    "        num_train = 0\n",
    "        num_accurate = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for data in self.train_data_list:\n",
    "            self.enc_model.zero_grad()\n",
    "            num_train+=1\n",
    "            \n",
    "            words = [token.norm for token in data.content_tokenized]\n",
    "            word_seqs = [self._vocab.get(token.norm, 0) for token in data.content_tokenized]\n",
    "            label = [self._label.get(data.label)]\n",
    "            #print(words[0])\n",
    "            #print(word_seqs)\n",
    "            #print(label_seq)\n",
    "            \n",
    "            logists = self.enc_model(word_seqs)\n",
    "            #print(logists)\n",
    "            loss = self.compute_loss(logists, label)\n",
    "            accurate = self.compute_accuracy(logists, label) \n",
    "            num_accurate += accurate\n",
    "            \n",
    "            #print(\"accuracy\", accuracy)\n",
    "            #print(\"loss\", loss.data[0])\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        print(\"Training loss: \", round(total_loss,2),  \"(\",num_train,\"/\",num_accurate.item(),\")\", \"  accuracy: \", round((num_accurate.item()/num_train)*100,2) )\n",
    "\n",
    "            \n",
    "    def compute_loss(self, pred_logist, gold):\n",
    "        \n",
    "        #gold = gold[0]\n",
    "        #gold = np.eye(3)[gold]\n",
    "        gold = torch.LongTensor(gold)\n",
    "        #print(gold)\n",
    "        #print(pred_logist.squeeze())\n",
    "        #print(F.softmax(pred_logist.squeeze()))\n",
    "        pred = pred_logist.unsqueeze(dim=0)\n",
    "        #print(pred)\n",
    "        #loss = F.cross_entropy(pred_logist.squeeze(), F.softmax(gold))\n",
    "        loss = self.criterion(pred, gold)\n",
    "        return loss\n",
    "    \n",
    "    def compute_accuracy(self, pred_logist, gold):\n",
    "        \n",
    "        #value, predicted = torch.max(pred_logist.data, 1)\n",
    "        #print(pred_logist.max())\n",
    "        #print(pred_logist.max(0))\n",
    "        pred = pred_logist.data.max(0)[1].cpu()\n",
    "        #print(\"predicted \" ,pred.data)\n",
    "        #print(\"predicted2 \" ,pred.data[0].cpu())\n",
    "        #print(\"gold \" ,gold[0])\n",
    "        predicted = (pred == gold[0]) if gold[0] is not None else 0\n",
    "        return predicted\n",
    "\n",
    "    def test(self, test_file=None):\n",
    "        print(\"Running test sets\")\n",
    "        self.enc_model.train()\n",
    "        \n",
    "        num_test = 0\n",
    "        num_accurate = 0\n",
    "        \n",
    "        test_data_list = read_fakenews(test_file) if self.test_data_list is None else self.test_data_list\n",
    "        #print(test_data_list[0].show())\n",
    "        for data in test_data_list:\n",
    "            num_test+=1\n",
    "            \n",
    "            words = [token.norm for token in data.content_tokenized]\n",
    "            word_seqs = [self._vocab.get(token.norm, 0) for token in data.content_tokenized]\n",
    "            label = [self._label.get(data.label)]\n",
    "            #print(label)\n",
    "\n",
    "            logists = self.enc_model(word_seqs)\n",
    "            #print(logists)\n",
    "            accurate = self.compute_accuracy(logists, label)\n",
    "            #print(accurate)\n",
    "            num_accurate += accurate\n",
    "\n",
    "        print(\"Testing results: \", \"(\",num_test,\"/\",num_accurate.item(),\")\", \"  accuracy: \", round((num_accurate.item()/num_test)*100,2) )\n",
    "\n",
    "        \n",
    "    def test_input(self, title, content):\n",
    "        print(\"Running with an input text\")\n",
    "        self.enc_model.eval()\n",
    "        \n",
    "        num_test = 0\n",
    "        num_accurate = 0\n",
    "        #input_data = FakeNews(line[0],line[1],line[2],line[3],line[4],line[5],line[6],line[7],line[8],line[9])\n",
    "        input_data = FakeNews(\"\",\"\",\"\",\"\",\"\",\"\",\"\",content,title,\"\")\n",
    "        input_data.tokenization()\n",
    "        \n",
    "        test_data_list = [input_data]\n",
    "        print(test_data_list[0].show())\n",
    "        for data in test_data_list:\n",
    "            num_test+=1\n",
    "            \n",
    "            words = [token.norm for token in data.content_tokenized]\n",
    "            word_seqs = [self._vocab.get(token.norm, 0) for token in data.content_tokenized]\n",
    "\n",
    "            logists = self.enc_model(word_seqs)\n",
    "            print(\"logists\", logists)\n",
    "            softLog = F.softmax(logists)\n",
    "            print(\"예측값:\", softLog)\n",
    "            #print(softLog.cpu()[0])\n",
    "            #print(softLog[1].item())\n",
    "            for idx, label in enumerate(self._ilabel):\n",
    "                print(label,\":\", round(softLog[idx].item(),3))\n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file:  ./corpus/train_ko.txt\n",
      "Finish to read:  10  sets\n",
      "Read file:  ./corpus/dev.txt\n",
      "build vocaburaries\n",
      "Vocab containing 1713 words\n",
      "Charset containing 586 chars\n",
      "UPOS containing 1 tags Counter({'Noun': 2386})\n",
      "LABEL containing 2 tags Counter({'fakeNews': 6, 'trusted': 4})\n"
     ]
    }
   ],
   "source": [
    "fakefinder = Fakefinder(\"./corpus/train_ko.txt\", \"./corpus/dev.txt\", n_read=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running train sets\n",
      "Training loss:  8.82 ( 10 / 10 )   accuracy:  100.0\n",
      "Running test sets\n",
      "Testing results:  ( 41 / 18 )   accuracy:  43.9\n",
      "Running with an input text\n",
      "ID:  \n",
      "Source:  \n",
      "Label:  \n",
      "Title:  \n",
      "Content:  [ 안혜원 기자 ] 지난해 나온 ‘9·13 부동산 대책’ 이후 서울·수도권 부동산시장이 힘을 쓰지 못하다가 이달 들어 서울 강남권 일부 단지가 반등에 성공하고 있다. 바닥을 확인했다는 분석과 반짝 반등에 불과하다는 의견이 대립하고 있다. 봄 성수기를 맞아 집값이 본격적으로 반등할 것인가, 더 떨어질 것인가. 한국경제신문과 한경닷컴은 이에 대한 해답을 얻기 위해 다음달 26일 최고의 부동산 전문가들을 초청해 ‘제2회 한경 집코노미 부동산 콘서트’를 연다. 콘서트 주제는 ‘올해 부동산 시장 어디로?’다. 다양한 견해를 듣기 위해 올해 집값이 강세를 보일 것으로 예상한 전문가와 약세일 것으로 전망한 전문가, 그리고 공시가격 급등이 집값에 미칠 영향을 분석할 세무 전문가를 강사로 초빙했다. 빅데이터 전문가와 함께 저평가 지역을 가려내는 토크쇼도 마련했다. 이상우 유진투자증권 연구위원이 첫 번째로 연단에 오른다. 이 연구위원은 2017년과 2018년 집값 상승률을 정확하게 예측해 부동산 시장의 ‘족집게’로 불린다. 올해도 8% 상승을 예상한 그는 이번 강연에선 ‘올해 부동산 투자의 핵심 키워드’를 짚어준다. 이어 원종훈 KB국민은행 세무팀장이 증여 등을 활용한 보유세 절세 전략을 강연한다. 주택임대사업자 등록에 따른 구체적 실익도 살펴볼 계획이다. 채상욱 하나금융투자 연구위원은 ‘규제의 시대 부동산 어떻게 해야 하나’를 주제로 강연한다. 채 연구위원은 올해 서울 집값이 8% 하락할 것으로 예상한 바 있다. 그는 약세장에서 지역과 아파트 면적, 공시가격 등에 따라 집값이 어떻게 차별화되는지 분석해 대응법을 제공한다. 마지막 순서인 토크쇼에선 빅데이터 전문가인 조영광 대우건설 연구원과 최진석 기자가 빅데이터를 활용해 저평가된 지역을 찾는 방법과 현재 저평가된 지역을 소개한다. 조 과장은 “적정가치 대비 싼값에 팔리는 부동산을 찾아볼 것”이라고 말했다. 행사는 논현동 건설회관 2층 대회의실에서 오후 1시부터 6시까지 열린다. 참가비는 5만5000원이다.\n",
      "None\n",
      "logists tensor([ 0.0922,  0.0172, -0.0381], grad_fn=<SqueezeBackward0>)\n",
      "예측값: tensor([0.3564, 0.3307, 0.3129], grad_fn=<SoftmaxBackward>)\n",
      "fakeNews : 0.356\n",
      "trusted : 0.331\n",
      "Running train sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/NLPApps/lib/python3.7/site-packages/ipykernel_launcher.py:167: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  8.41 ( 10 / 10 )   accuracy:  100.0\n",
      "Running test sets\n",
      "Testing results:  ( 41 / 18 )   accuracy:  43.9\n",
      "Running with an input text\n",
      "ID:  \n",
      "Source:  \n",
      "Label:  \n",
      "Title:  \n",
      "Content:  [ 안혜원 기자 ] 지난해 나온 ‘9·13 부동산 대책’ 이후 서울·수도권 부동산시장이 힘을 쓰지 못하다가 이달 들어 서울 강남권 일부 단지가 반등에 성공하고 있다. 바닥을 확인했다는 분석과 반짝 반등에 불과하다는 의견이 대립하고 있다. 봄 성수기를 맞아 집값이 본격적으로 반등할 것인가, 더 떨어질 것인가. 한국경제신문과 한경닷컴은 이에 대한 해답을 얻기 위해 다음달 26일 최고의 부동산 전문가들을 초청해 ‘제2회 한경 집코노미 부동산 콘서트’를 연다. 콘서트 주제는 ‘올해 부동산 시장 어디로?’다. 다양한 견해를 듣기 위해 올해 집값이 강세를 보일 것으로 예상한 전문가와 약세일 것으로 전망한 전문가, 그리고 공시가격 급등이 집값에 미칠 영향을 분석할 세무 전문가를 강사로 초빙했다. 빅데이터 전문가와 함께 저평가 지역을 가려내는 토크쇼도 마련했다. 이상우 유진투자증권 연구위원이 첫 번째로 연단에 오른다. 이 연구위원은 2017년과 2018년 집값 상승률을 정확하게 예측해 부동산 시장의 ‘족집게’로 불린다. 올해도 8% 상승을 예상한 그는 이번 강연에선 ‘올해 부동산 투자의 핵심 키워드’를 짚어준다. 이어 원종훈 KB국민은행 세무팀장이 증여 등을 활용한 보유세 절세 전략을 강연한다. 주택임대사업자 등록에 따른 구체적 실익도 살펴볼 계획이다. 채상욱 하나금융투자 연구위원은 ‘규제의 시대 부동산 어떻게 해야 하나’를 주제로 강연한다. 채 연구위원은 올해 서울 집값이 8% 하락할 것으로 예상한 바 있다. 그는 약세장에서 지역과 아파트 면적, 공시가격 등에 따라 집값이 어떻게 차별화되는지 분석해 대응법을 제공한다. 마지막 순서인 토크쇼에선 빅데이터 전문가인 조영광 대우건설 연구원과 최진석 기자가 빅데이터를 활용해 저평가된 지역을 찾는 방법과 현재 저평가된 지역을 소개한다. 조 과장은 “적정가치 대비 싼값에 팔리는 부동산을 찾아볼 것”이라고 말했다. 행사는 논현동 건설회관 2층 대회의실에서 오후 1시부터 6시까지 열린다. 참가비는 5만5000원이다.\n",
      "None\n",
      "logists tensor([ 0.1223,  0.0122, -0.0621], grad_fn=<SqueezeBackward0>)\n",
      "예측값: tensor([0.3666, 0.3284, 0.3049], grad_fn=<SoftmaxBackward>)\n",
      "fakeNews : 0.367\n",
      "trusted : 0.328\n",
      "Running train sets\n",
      "Training loss:  8.03 ( 10 / 10 )   accuracy:  100.0\n",
      "Running test sets\n",
      "Testing results:  ( 41 / 18 )   accuracy:  43.9\n",
      "Running with an input text\n",
      "ID:  \n",
      "Source:  \n",
      "Label:  \n",
      "Title:  \n",
      "Content:  [ 안혜원 기자 ] 지난해 나온 ‘9·13 부동산 대책’ 이후 서울·수도권 부동산시장이 힘을 쓰지 못하다가 이달 들어 서울 강남권 일부 단지가 반등에 성공하고 있다. 바닥을 확인했다는 분석과 반짝 반등에 불과하다는 의견이 대립하고 있다. 봄 성수기를 맞아 집값이 본격적으로 반등할 것인가, 더 떨어질 것인가. 한국경제신문과 한경닷컴은 이에 대한 해답을 얻기 위해 다음달 26일 최고의 부동산 전문가들을 초청해 ‘제2회 한경 집코노미 부동산 콘서트’를 연다. 콘서트 주제는 ‘올해 부동산 시장 어디로?’다. 다양한 견해를 듣기 위해 올해 집값이 강세를 보일 것으로 예상한 전문가와 약세일 것으로 전망한 전문가, 그리고 공시가격 급등이 집값에 미칠 영향을 분석할 세무 전문가를 강사로 초빙했다. 빅데이터 전문가와 함께 저평가 지역을 가려내는 토크쇼도 마련했다. 이상우 유진투자증권 연구위원이 첫 번째로 연단에 오른다. 이 연구위원은 2017년과 2018년 집값 상승률을 정확하게 예측해 부동산 시장의 ‘족집게’로 불린다. 올해도 8% 상승을 예상한 그는 이번 강연에선 ‘올해 부동산 투자의 핵심 키워드’를 짚어준다. 이어 원종훈 KB국민은행 세무팀장이 증여 등을 활용한 보유세 절세 전략을 강연한다. 주택임대사업자 등록에 따른 구체적 실익도 살펴볼 계획이다. 채상욱 하나금융투자 연구위원은 ‘규제의 시대 부동산 어떻게 해야 하나’를 주제로 강연한다. 채 연구위원은 올해 서울 집값이 8% 하락할 것으로 예상한 바 있다. 그는 약세장에서 지역과 아파트 면적, 공시가격 등에 따라 집값이 어떻게 차별화되는지 분석해 대응법을 제공한다. 마지막 순서인 토크쇼에선 빅데이터 전문가인 조영광 대우건설 연구원과 최진석 기자가 빅데이터를 활용해 저평가된 지역을 찾는 방법과 현재 저평가된 지역을 소개한다. 조 과장은 “적정가치 대비 싼값에 팔리는 부동산을 찾아볼 것”이라고 말했다. 행사는 논현동 건설회관 2층 대회의실에서 오후 1시부터 6시까지 열린다. 참가비는 5만5000원이다.\n",
      "None\n",
      "logists tensor([ 0.1515,  0.0068, -0.0848], grad_fn=<SqueezeBackward0>)\n",
      "예측값: tensor([0.3767, 0.3259, 0.2974], grad_fn=<SoftmaxBackward>)\n",
      "fakeNews : 0.377\n",
      "trusted : 0.326\n",
      "Running train sets\n",
      "Training loss:  7.65 ( 10 / 10 )   accuracy:  100.0\n",
      "Running test sets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-33190c062389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfakefinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfakefinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     fakefinder.test_input(\"부동산시장 어디로?…'집코노미 콘서트'에서 해답찾으세요.\",\n\u001b[1;32m      6\u001b[0m                           \"[ 안혜원 기자 ] 지난해 나온 ‘9·13 부동산 대책’ 이후 서울·수도권 부동산시장이 힘을 쓰지 못하다가 이달 들어 서울 강남권 일부 단지가 반등에 성공하고 있다. 바닥을 확인했다는 분석과 반짝 반등에 불과하다는 의견이 대립하고 있다. 봄 성수기를 맞아 집값이 본격적으로 반등할 것인가, 더 떨어질 것인가. 한국경제신문과 한경닷컴은 이에 대한 해답을 얻기 위해 다음달 26일 최고의 부동산 전문가들을 초청해 ‘제2회 한경 집코노미 부동산 콘서트’를 연다. 콘서트 주제는 ‘올해 부동산 시장 어디로?’다. 다양한 견해를 듣기 위해 올해 집값이 강세를 보일 것으로 예상한 전문가와 약세일 것으로 전망한 전문가, 그리고 공시가격 급등이 집값에 미칠 영향을 분석할 세무 전문가를 강사로 초빙했다. 빅데이터 전문가와 함께 저평가 지역을 가려내는 토크쇼도 마련했다. 이상우 유진투자증권 연구위원이 첫 번째로 연단에 오른다. 이 연구위원은 2017년과 2018년 집값 상승률을 정확하게 예측해 부동산 시장의 ‘족집게’로 불린다. 올해도 8% 상승을 예상한 그는 이번 강연에선 ‘올해 부동산 투자의 핵심 키워드’를 짚어준다. 이어 원종훈 KB국민은행 세무팀장이 증여 등을 활용한 보유세 절세 전략을 강연한다. 주택임대사업자 등록에 따른 구체적 실익도 살펴볼 계획이다. 채상욱 하나금융투자 연구위원은 ‘규제의 시대 부동산 어떻게 해야 하나’를 주제로 강연한다. 채 연구위원은 올해 서울 집값이 8% 하락할 것으로 예상한 바 있다. 그는 약세장에서 지역과 아파트 면적, 공시가격 등에 따라 집값이 어떻게 차별화되는지 분석해 대응법을 제공한다. 마지막 순서인 토크쇼에선 빅데이터 전문가인 조영광 대우건설 연구원과 최진석 기자가 빅데이터를 활용해 저평가된 지역을 찾는 방법과 현재 저평가된 지역을 소개한다. 조 과장은 “적정가치 대비 싼값에 팔리는 부동산을 찾아볼 것”이라고 말했다. 행사는 논현동 건설회관 2층 대회의실에서 오후 1시부터 6시까지 열린다. 참가비는 5만5000원이다.\")\n",
      "\u001b[0;32m<ipython-input-7-f234209ebe42>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, test_file)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m#print(label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mlogists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;31m#print(logists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0maccurate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/NLPApps/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-37a25ac274cc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_seqs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0minput_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_lstm_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(lstm_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print(lstm_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/NLPApps/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/NLPApps/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "while range(100):\n",
    "    fakefinder.train()\n",
    "    fakefinder.test()\n",
    "    fakefinder.test_input(\"부동산시장 어디로?…'집코노미 콘서트'에서 해답찾으세요.\",\n",
    "                          \"[ 안혜원 기자 ] 지난해 나온 ‘9·13 부동산 대책’ 이후 서울·수도권 부동산시장이 힘을 쓰지 못하다가 이달 들어 서울 강남권 일부 단지가 반등에 성공하고 있다. 바닥을 확인했다는 분석과 반짝 반등에 불과하다는 의견이 대립하고 있다. 봄 성수기를 맞아 집값이 본격적으로 반등할 것인가, 더 떨어질 것인가. 한국경제신문과 한경닷컴은 이에 대한 해답을 얻기 위해 다음달 26일 최고의 부동산 전문가들을 초청해 ‘제2회 한경 집코노미 부동산 콘서트’를 연다. 콘서트 주제는 ‘올해 부동산 시장 어디로?’다. 다양한 견해를 듣기 위해 올해 집값이 강세를 보일 것으로 예상한 전문가와 약세일 것으로 전망한 전문가, 그리고 공시가격 급등이 집값에 미칠 영향을 분석할 세무 전문가를 강사로 초빙했다. 빅데이터 전문가와 함께 저평가 지역을 가려내는 토크쇼도 마련했다. 이상우 유진투자증권 연구위원이 첫 번째로 연단에 오른다. 이 연구위원은 2017년과 2018년 집값 상승률을 정확하게 예측해 부동산 시장의 ‘족집게’로 불린다. 올해도 8% 상승을 예상한 그는 이번 강연에선 ‘올해 부동산 투자의 핵심 키워드’를 짚어준다. 이어 원종훈 KB국민은행 세무팀장이 증여 등을 활용한 보유세 절세 전략을 강연한다. 주택임대사업자 등록에 따른 구체적 실익도 살펴볼 계획이다. 채상욱 하나금융투자 연구위원은 ‘규제의 시대 부동산 어떻게 해야 하나’를 주제로 강연한다. 채 연구위원은 올해 서울 집값이 8% 하락할 것으로 예상한 바 있다. 그는 약세장에서 지역과 아파트 면적, 공시가격 등에 따라 집값이 어떻게 차별화되는지 분석해 대응법을 제공한다. 마지막 순서인 토크쇼에선 빅데이터 전문가인 조영광 대우건설 연구원과 최진석 기자가 빅데이터를 활용해 저평가된 지역을 찾는 방법과 현재 저평가된 지역을 소개한다. 조 과장은 “적정가치 대비 싼값에 팔리는 부동산을 찾아볼 것”이라고 말했다. 행사는 논현동 건설회관 2층 대회의실에서 오후 1시부터 6시까지 열린다. 참가비는 5만5000원이다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
